{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# read example raw data from package and store to hdf5 file\n",
    "\n",
    "the following provides a minimal working example of how to use DataLoader.read_flat_files to read raw data from filesystem\n",
    "\n",
    "### to read in data and store via command line one can use:\n",
    "\n",
    "```commandline\n",
    "python -m PyOptimalInterpolation.read_and_store /path/to/config.json\n",
    "```\n",
    "\n",
    "ensure an appropriate python environment is activated e.g. one satisfying requirements.txt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## package import"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# need to add parent directory to sys.path...\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.getcwd()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from PyOptimalInterpolation import get_data_path, get_config_path\n",
    "from PyOptimalInterpolation.dataloader import DataLoader\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## read config and specify directory containing data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading raw data and storing to hdf file using config:\n",
      "{\n",
      "    \"output\": {\n",
      "        \"dir\": \"/home/buddy/workspace/sparse_opt_interp/PyOptimalInterpolation/data/example\",\n",
      "        \"file\": \"ABC.h5\",\n",
      "        \"table\": \"data\",\n",
      "        \"append\": false\n",
      "    },\n",
      "    \"file_dirs\": \"/home/buddy/workspace/sparse_opt_interp/PyOptimalInterpolation/data/example\",\n",
      "    \"file_regex\": \"\\\\.csv$\",\n",
      "    \"sub_dirs\": null,\n",
      "    \"read_csv_kwargs\": {},\n",
      "    \"col_funcs\": {\n",
      "        \"source\": {\n",
      "            \"func\": \"lambda x: re.sub('_RAW.*$', '', os.path.basename(x))\",\n",
      "            \"filename_as_arg\": true\n",
      "        },\n",
      "        \"datetime\": {\n",
      "            \"func\": \"lambda x: x.astype('datetime64[s]')\",\n",
      "            \"col_args\": \"datetime\"\n",
      "        },\n",
      "        \"obs\": {\n",
      "            \"func\": \"lambda x,y: x-y\",\n",
      "            \"col_args\": [\n",
      "                \"z\",\n",
      "                \"z_mean\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"row_select\": [\n",
      "        {\n",
      "            \"func\": \"lambda x: x>=65\",\n",
      "            \"col_kwargs\": {\n",
      "                \"x\": \"lat\"\n",
      "            }\n",
      "        }\n",
      "    ],\n",
      "    \"col_select\": [\n",
      "        \"lon\",\n",
      "        \"lat\",\n",
      "        \"datetime\",\n",
      "        \"source\",\n",
      "        \"obs\"\n",
      "    ],\n",
      "    \"verbose\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# (example) data path - in package\n",
    "data_dir = get_data_path(\"example\")\n",
    "\n",
    "# configuration file to read data\n",
    "config_file = get_config_path(\"example_read_and_store_raw_data.json\")\n",
    "\n",
    "with open(config_file, \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# change some of the directory locations to the package\n",
    "config['output']['dir'] = data_dir\n",
    "config['file_dirs'] = data_dir\n",
    "\n",
    "print(\"reading raw data and storing to hdf file using config:\")\n",
    "print(json.dumps(config, indent=4))\n",
    "\n",
    "# extract (pop out) the output information\n",
    "output_dict = config.pop(\"output\", None)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## read in data, select rows and columns, combine into a single dataframe\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "reading files from:\n",
      "/home/buddy/workspace/sparse_opt_interp/PyOptimalInterpolation/data/example/\n",
      "read in raw data, looks like:\n"
     ]
    },
    {
     "data": {
      "text/plain": "           lon        lat            datetime source      obs\n420 -61.821182  65.594139 2020-02-10 00:22:20      A  0.22610\n421 -61.847626  65.620234 2020-02-10 00:22:23      A  0.21440\n422 -61.851806  65.624354 2020-02-10 00:22:24      A  0.09765\n423 -61.878307  65.650444 2020-02-10 00:22:27      A  0.20980\n424 -61.885290  65.657310 2020-02-10 00:22:28      A  0.21360",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lon</th>\n      <th>lat</th>\n      <th>datetime</th>\n      <th>source</th>\n      <th>obs</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>420</th>\n      <td>-61.821182</td>\n      <td>65.594139</td>\n      <td>2020-02-10 00:22:20</td>\n      <td>A</td>\n      <td>0.22610</td>\n    </tr>\n    <tr>\n      <th>421</th>\n      <td>-61.847626</td>\n      <td>65.620234</td>\n      <td>2020-02-10 00:22:23</td>\n      <td>A</td>\n      <td>0.21440</td>\n    </tr>\n    <tr>\n      <th>422</th>\n      <td>-61.851806</td>\n      <td>65.624354</td>\n      <td>2020-02-10 00:22:24</td>\n      <td>A</td>\n      <td>0.09765</td>\n    </tr>\n    <tr>\n      <th>423</th>\n      <td>-61.878307</td>\n      <td>65.650444</td>\n      <td>2020-02-10 00:22:27</td>\n      <td>A</td>\n      <td>0.20980</td>\n    </tr>\n    <tr>\n      <th>424</th>\n      <td>-61.885290</td>\n      <td>65.657310</td>\n      <td>2020-02-10 00:22:28</td>\n      <td>A</td>\n      <td>0.21360</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = DataLoader.read_flat_files(**config)\n",
    "\n",
    "print(\"read in raw data, looks like:\")\n",
    "df.head(5)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## store as hdf5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing to hdf5 file\n"
     ]
    }
   ],
   "source": [
    "# get run information (including some details from git)\n",
    "# - for auditing / future reference\n",
    "# run_info = DataLoader.get_run_info()\n",
    "\n",
    "# or provide some custom one\n",
    "run_info = {\n",
    "    \"run_time\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "}\n",
    "\n",
    "# specify output dir, file, table name and whether to append or not\n",
    "output_dir = output_dict['dir']\n",
    "out_file = output_dict['file']\n",
    "table = output_dict['table']\n",
    "append = output_dict.get(\"append\", False)\n",
    "\n",
    "print(\"writing to hdf5 file\")\n",
    "with pd.HDFStore(path=os.path.join(output_dir, out_file), mode='a' if append else 'w') as store:\n",
    "    DataLoader.write_to_hdf(df,\n",
    "                            table=table,\n",
    "                            append=append,\n",
    "                            store=store,\n",
    "                            config=config,\n",
    "                            run_info=run_info)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"complete\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
